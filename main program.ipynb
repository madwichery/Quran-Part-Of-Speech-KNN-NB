{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c3b8d7f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.metrics'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [22]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mextended_buckwalter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transliterate, remove_diacritics\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspatial\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistance\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m hamming\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m accuracy_score\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'sklearn.metrics'"
     ]
    }
   ],
   "source": [
    "import re, numpy as np, pandas as pd, time\n",
    "from collections import Counter\n",
    "from extended_buckwalter import transliterate, remove_diacritics\n",
    "from scipy.spatial.distance import hamming\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45860d32",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pick = '2'\n",
    "\n",
    "f = open('properties.txt').read()\n",
    "\n",
    "prefix, suffix, labels = [], [], []\n",
    "for x, i in enumerate(f.splitlines()[3:]):\n",
    "    i = re.sub('[ ][ ]+', ' ', i)\n",
    "    locs, tags, mtag, word, form, rems, prfx, sffx = i.split(' | ')\n",
    "\n",
    "    if not locs.startswith(pick + ':'): continue\n",
    "\n",
    "    prfx = ''.join(prfx.split())\n",
    "    sffx = ''.join(sffx.split())\n",
    "\n",
    "    prefix.append(prfx if prfx != '-' else None)\n",
    "    suffix.append(sffx if sffx != '-' else None)\n",
    "    labels.append(tags.split()[0])\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "trans = open('transliterasi.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c54154",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "I = 0\n",
    "data = []\n",
    "for sents in trans.splitlines():\n",
    "\n",
    "    if not sents.startswith(pick + ' |'): continue\n",
    "\n",
    "    sents = sents.split(' | ')[-1].split()\n",
    "    lensents = len(sents)\n",
    "\n",
    "    for i, word in enumerate(sents):\n",
    "\n",
    "        fwpos = [word, labels[I]]\n",
    "\n",
    "        fword, fpost = [], []\n",
    "        for v in range(-3, 4):\n",
    "            if i + v >= 0 and i + v < lensents:\n",
    "                fword.append(sents[i + v])\n",
    "                fpost.append(labels[I + v])\n",
    "            else:\n",
    "                fword.append('-')\n",
    "                fpost.append('-')\n",
    "\n",
    "        fprfx, temp = [], []\n",
    "        for v in range(1, 5):\n",
    "            if i + v < lensents:\n",
    "                if prefix[I + v] != None:\n",
    "                    temp.append(prefix[I + v])\n",
    "                else:\n",
    "                    temp.append('')\n",
    "            else:\n",
    "                temp.append('')\n",
    "            vals = ''.join(temp)\n",
    "            fprfx.append(vals if vals != '' else '-')\n",
    "\n",
    "        fsffx, temp = [], []\n",
    "        for v in range(0, -4, -1):\n",
    "            if i + v >= 0:\n",
    "                if suffix[I + v] != None:\n",
    "                    temp.append(suffix[I + v])\n",
    "                else:\n",
    "                    temp.append('')\n",
    "            else:\n",
    "                temp.append('')\n",
    "            vals = ''.join(temp)\n",
    "            fsffx.append(vals if vals != '' else '-')\n",
    "\n",
    "        '''\n",
    "        fbins = [True if word.isupper() else False]\n",
    "        fbins.append(True if word.islower() else False)\n",
    "\n",
    "        x = re.findall('\\d+',word)\n",
    "        fbins.append(True if x != [] else False)\n",
    "        '''\n",
    "\n",
    "        fbins = ['False', 'False']\n",
    "        x = re.findall('\\d+', word)\n",
    "        fbins.append('True' if x != [] else 'False')\n",
    "\n",
    "        flens = [str(len(remove_diacritics(word)))]\n",
    "\n",
    "        feature = fwpos + fword + fpost + fprfx + fsffx + fbins + flens\n",
    "        data.append(feature)\n",
    "\n",
    "        I += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f219e079",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = np.asarray(data)\n",
    "\n",
    "arab = np.copy(data)\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[0])):\n",
    "        if j in [0, 2, 3, 4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 21, 22, 23]:\n",
    "            if data[i][j] in ['-', 'none']:\n",
    "                continue\n",
    "            trans = data[i][j].replace('none', '؟؟؟؟')\n",
    "            trans = transliterate(trans)\n",
    "            trans = trans.replace('؟؟؟؟', 'none')\n",
    "            arab[i, j] = trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f6b78",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = \"\"\"\n",
    "WORD POS\n",
    "W-3 W-2 W-1 W0 W1 W2 W3\n",
    "P-3 P-2 P-1 P0 P1 P2 P3\n",
    "S1 S1,S2 S1,S2,S3 S1,S2,S3,S4\n",
    "S0 S0,S-1 S0,S-1,S-2 S0,S-1,S-2,S-3\n",
    "uppercase lowercase contain_number\n",
    "length\n",
    "\"\"\".split()\n",
    "\n",
    "writer = pd.ExcelWriter('feature.xlsx')\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = cols\n",
    "df.to_excel(writer, sheet_name='transliterasi')\n",
    "\n",
    "df = pd.DataFrame(arab)\n",
    "df.columns = cols\n",
    "df.to_excel(writer, sheet_name='arabic')\n",
    "writer.save()\n",
    "\n",
    "data_ori = np.copy(data)\n",
    "data = np.delete(data, [3, 4, 5, 6, 7, 8, 9], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd65c8b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = \"\"\"\n",
    "W-3 W-2 W-1 W0 W1 W2 W3\n",
    "P-3 P-2 P-1 P0 P1 P2 P3\n",
    "S1 S1,S2 S1,S2,S3 S1,S2,S3,S4\n",
    "S0 S0,S-1 S0,S-1,S-2 S0,S-1,S-2,S-3\n",
    "length\n",
    "\"\"\".split()\n",
    "\n",
    "Freq = Counter(labels)\n",
    "\n",
    "data_train, label_train = [], []\n",
    "data_test, label_test = [], []\n",
    "\n",
    "data_train_ori, data_test_ori = [], []\n",
    "\n",
    "Itrain, Itest = [], []\n",
    "\n",
    "for i in sorted(set(labels)):\n",
    "    n = Freq[i]\n",
    "    m = int(np.ceil(n / 2))\n",
    "\n",
    "    index_clas = np.arange(len(data))[labels == i]\n",
    "\n",
    "    index_train = np.sort(np.random.choice(index_clas, m, replace=False))\n",
    "    index_test = np.asarray([j for j in index_clas if j not in index_train])\n",
    "\n",
    "    for j in index_train:\n",
    "        data_train.append(data[j])\n",
    "        data_train_ori.append(data_ori[j])\n",
    "        label_train.append(labels[j])\n",
    "    for j in index_test:\n",
    "        data_test.append(data[j])\n",
    "        data_test_ori.append(data_ori[j])\n",
    "        label_test.append(labels[j])\n",
    "\n",
    "    Itrain += index_train.tolist()\n",
    "    Itest += index_test.tolist()\n",
    "\n",
    "data_train = np.asarray(data_train)\n",
    "data_test = np.asarray(data_test)\n",
    "label_train = np.asarray(label_train)\n",
    "label_test = np.asarray(label_test)\n",
    "\n",
    "data_train_ori = np.asarray(data_train_ori)\n",
    "data_test_ori = np.asarray(data_test_ori)\n",
    "\n",
    "writer = pd.ExcelWriter('data_train_test.xlsx')\n",
    "\n",
    "df = pd.DataFrame(data_train)\n",
    "df.columns = cols\n",
    "df.index = Itrain\n",
    "df.to_excel(writer, sheet_name='data train')\n",
    "\n",
    "df = pd.DataFrame(data_test)\n",
    "df.columns = cols\n",
    "df.index = Itest\n",
    "df.to_excel(writer, sheet_name='data test')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e267477",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(data_train, label_train, data_test):\n",
    "    K = 7\n",
    "    N = len(data_test)\n",
    "\n",
    "    label_predict = []\n",
    "    for i, data_i in enumerate(data_test):\n",
    "\n",
    "        dist = []\n",
    "        for data_j in data_train:\n",
    "            dist.append(hamming(data_i, data_j))\n",
    "\n",
    "        sort = np.argsort(dist)\n",
    "\n",
    "        I = 0\n",
    "        while True:\n",
    "\n",
    "            post = label_train[sort[:K + I]]\n",
    "            freq = Counter(post)\n",
    "\n",
    "            tags, maxf = freq.most_common(1)[0]\n",
    "\n",
    "            if list(freq.values()).count(maxf) == 1: break\n",
    "            I += 1\n",
    "\n",
    "        label_predict.append(tags)\n",
    "\n",
    "    return label_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc2c7f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def naive_bayes(data_train, label_train, data_test):\n",
    "    N = len(data_train)\n",
    "    V = len(data_train[0])\n",
    "\n",
    "    clas = sorted(set(label_train))\n",
    "    Freq = Counter(label_train)\n",
    "\n",
    "    P_label = {}\n",
    "    for pos in clas:\n",
    "        P_label[pos] = Freq[pos] / N\n",
    "\n",
    "    P_xiyj = {}\n",
    "    for i in range(V):\n",
    "        cols = data_train[:, i]\n",
    "        setcols = sorted(set(cols))\n",
    "        for j, word in enumerate(setcols):\n",
    "\n",
    "            freq = Counter(label_train[cols == word])\n",
    "\n",
    "            for pos in clas:\n",
    "                key = word + '|' + str(i) + '|' + pos\n",
    "                P_xiyj[key] = freq[pos] / Freq[pos]\n",
    "\n",
    "    label_predict = []\n",
    "    for data_i in data_test:\n",
    "\n",
    "        P_i = []\n",
    "        for pos in clas:\n",
    "\n",
    "            P_ij = P_label[pos]\n",
    "            for i, word in enumerate(data_i):\n",
    "\n",
    "                key = word + '|' + str(i) + '|' + pos\n",
    "\n",
    "                try:\n",
    "                    P_xy = P_xiyj[key]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "                P_ij = P_ij * P_xy\n",
    "\n",
    "            P_i.append(P_ij)\n",
    "\n",
    "        label_predict.append(clas[np.argmax(P_i)])\n",
    "    return label_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b13b9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('result.xlsx')\n",
    "\n",
    "print('>> NAIVE BAYES: ')\n",
    "start = time.time()\n",
    "predict_nb = naive_bayes(data_train, label_train, data_test)\n",
    "accur = accuracy_score(predict_nb, label_test)\n",
    "print('   ACCURACY:', str(np.round(accur * 100, 2)) + '%')\n",
    "print('   RUNTIME :', time.time() - start, 'detik\\n')\n",
    "\n",
    "df = pd.DataFrame(np.asarray([list(i[:2]) + [j] for i, j in zip(data_test_ori, predict_nb)]))\n",
    "df.columns = ['WORD', 'POS_ASLI', 'POS_HASIL']\n",
    "df.index = Itest\n",
    "df.to_excel(writer, sheet_name='naive bayes')\n",
    "\n",
    "print('>> K-NEAREST NEIGHBORS:')\n",
    "start = time.time()\n",
    "predict_knn = k_nearest_neighbors(data_train, label_train, data_test)\n",
    "accur = accuracy_score(predict_knn, label_test)\n",
    "print('   ACCURACY:', str(np.round(accur * 100, 2)) + '%')\n",
    "print('   RUNTIME :', time.time() - start, 'detik\\n')\n",
    "\n",
    "df = pd.DataFrame(np.asarray([list(i[:2]) + [j] for i, j in zip(data_test_ori, predict_knn)]))\n",
    "df.columns = ['WORD', 'POS_ASLI', 'POS_HASIL']\n",
    "df.index = Itest\n",
    "df.to_excel(writer, sheet_name='knn')\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef13b94",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}